{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 248\r\n",
      "-rw-r--r--  1 zhansu  staff   2351  6 13 21:31 README.md\r\n",
      "drwxr-xr-x  5 zhansu  staff    160  6 14 14:47 \u001b[34m__pycache__\u001b[m\u001b[m/\r\n",
      "-rw-r--r--  1 zhansu  staff   2374  6 14 14:39 config.py\r\n",
      "-rw-r--r--  1 zhansu  staff   2245  6 13 21:43 config.pyc\r\n",
      "drwxr-xr-x  5 zhansu  staff    160  6 13 21:44 \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "-rw-r--r--  1 zhansu  staff  12740  6 14 14:47 dataset.py\r\n",
      "-rw-r--r--  1 zhansu  staff   4418  6 13 21:48 evaluation.py\r\n",
      "drwxr-xr-x  3 zhansu  staff     96  6  6 10:24 \u001b[34mfigure\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x  6 zhansu  staff    192  6  6 10:24 \u001b[34mmodel\u001b[m\u001b[m/\r\n",
      "-rw-r--r--  1 zhansu  staff  33716  6 14 14:51 pred.txt\r\n",
      "-rw-r--r--  1 zhansu  staff  24878  6 27 22:08 qa.ipynb\r\n",
      "-rw-r--r--  1 zhansu  staff     57  6 13 21:22 requirements.txt\r\n",
      "-rw-r--r--  1 zhansu  staff   6684  6 14 14:47 run.py\r\n",
      "-rw-r--r--  1 zhansu  staff    238  6 13 21:22 test.py\r\n",
      "-rw-r--r--  1 zhansu  staff   4541  6 27 20:22 未命名.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-62f2da51e945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/wiki'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m's1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m's2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'flag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquoting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 将s1和s2合并构成一句话变成文本分类问题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "data = 'data/wiki'\n",
    "train_file = os.path.join(data,'train.txt')\n",
    "test_file = os.path.join(data,'test.txt')\n",
    "df_train = pd.read_csv(train_file,sep = '\\t',names = ['s1','s2','flag'],quoting = 3)\n",
    "# 将s1和s2合并构成一句话变成文本分类问题\n",
    "df_train['sentence'] = df_train['s1'] + df_train['s2']\n",
    "df_test = pd.read_csv(test_file,sep = '\\t',names = ['s1','s2','flag'],quoting = 3)\n",
    "df_test['sentence'] = df_test['s1'] + df_test['s2']\n",
    "df_test = df_test.sample(frac = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-788ec8190561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(df_train['sentence'])\n",
    "# print('X_train_counts:{}'.format(X_train_counts.shape))\n",
    "# X_test_counts = count_vect.transform(df_test['sentence'])\n",
    "# print('X_test_counts:{}'.format(X_test_counts.shape))\n",
    "\n",
    "# # use the tf_idf Transform\n",
    "# tfidf_transform = TfidfTransformer()\n",
    "# tf_idf_train = tfidf_transform.fit_transform(X_train_counts)\n",
    "# print('tf_idf_train:{}'.format(tf_idf_train.shape))\n",
    "# tf_idf_test = tfidf_transform.transform(X_test_counts)\n",
    "# print('tf_idf_test:{}'.format(tf_idf_test.shape))\n",
    "# # logistic regression\n",
    "# reg = linear_model.LogisticRegression()\n",
    "# reg.fit(tf_idf_train,df_train['flag'])\n",
    "\n",
    "# pred = reg.predict(tf_idf_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluation the map and mrr\n",
    "# print(pred)\n",
    "# # print(len(df_test['flag']))\n",
    "# print(np.mean(pred == df_test['flag']))\n",
    "\n",
    "# # evaluation the map\n",
    "# import evaluation\n",
    "\n",
    "# random_map = np.random.randn(len(pred))\n",
    "# random_map[random_map < 0.5] = 0\n",
    "# random_map[random_map >= 0.5] = 1\n",
    "# print(np.mean(random_map == df_test['flag']))\n",
    "# print(evaluation.evaluationBypandas(df_test,random_map))\n",
    "\n",
    "# print(evaluation.evaluationBypandas(df_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据集合\n",
    "\n",
    "def gen_test(infile,outfile):\n",
    "    with open(outfile,'w') as fo:\n",
    "        for line in open(infile):\n",
    "            q,a,l = line.strip().split('\\t')\n",
    "            fo.write(q + '\\t' + a + '\\n')\n",
    "            \n",
    "def gen_train(infile, outfile):\n",
    "    \"\"\"\n",
    "    extract question answer pairs which has right answer (label 1) and gen triple\n",
    "    (question, positive answer, negative answer)\n",
    "    \"\"\"\n",
    "\n",
    "    ############# 生成 q 的 a和label 字典  #############\n",
    "    with open(infile) as fi:\n",
    "        qa_pairs = {}\n",
    "        for line in fi:\n",
    "            q, a, label = line.split(\"\\t\")\n",
    "            label = int(label)\n",
    "            if q in qa_pairs:\n",
    "                qa_pairs[q].append((a, label))\n",
    "            else:\n",
    "                qa_pairs[q] = [(a, label)]\n",
    "\n",
    "\n",
    "    ############# 将label存在1的 q 生成(q, a+, a-)，输出 #############\n",
    "    with open(outfile, \"w\") as fo:\n",
    "        for q in qa_pairs:\n",
    "            pos_ans = set()\n",
    "            neg_ans = set()\n",
    "            for a, label in qa_pairs[q]:\n",
    "                if label == 1:\n",
    "                    pos_ans.add(a)\n",
    "                else:\n",
    "                    neg_ans.add(a)\n",
    "\n",
    "            if pos_ans:  # has answer\n",
    "                for a1 in pos_ans:\n",
    "                    for a2 in neg_ans:\n",
    "                        triple = [q, a1, a2]\n",
    "                        fo.write('\\t'.join(triple) + '\\n')\n",
    "                        \n",
    "gen_test(test_file,os.path.join(data,'test'))\n",
    "gen_train(train_file,os.path.join(data,'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab\n",
    "from collections import Counter\n",
    "def build_vocab(infile,outfile,max_vocab_size = None):\n",
    "    tokens = []\n",
    "    word_dict = {}\n",
    "    for line in open(infile):\n",
    "        line = ' '.join(line.strip().lower().split('\\t')).split(' ')\n",
    "        for token in line:\n",
    "            tokens.append(token)\n",
    "        \n",
    "    counter = Counter(tokens)\n",
    "    \n",
    "    \n",
    "#     word_count = counter.most_common(max_vocab_size)\n",
    "        \n",
    "    word_dict = {w:e + 2 for e,w in enumerate(list(counter))}\n",
    "    word_dict['UNK'] = 0\n",
    "    word_dict['<PAD>'] = 1\n",
    "#     with open(outfile,'w') as fo:\n",
    "#         for w,e in enumerate(vocab):\n",
    "#             word_dict[w] = e + 2\n",
    "#             fo.write(w + '\\n')\n",
    "    return word_dict\n",
    "\n",
    "word_dict = build_vocab(train_file,os.path.join(data,'vocab'))\n",
    "# build vocab_file\n",
    "# build_vocab(train_file,os.path.join(data,'vocab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../glove.6B.100d.txt\n",
      "epch 100000\n",
      "epch 200000\n",
      "epch 300000\n",
      "epch 400000\n",
      "there are 30821 words can be found in dict\n"
     ]
    }
   ],
   "source": [
    "# load embedding\n",
    "import numpy as np\n",
    "def get_embedding(fname,vocab,dim = 100):\n",
    "        print(fname)\n",
    "        embeddings = np.random.normal(0,1,size = [len(vocab),dim])\n",
    "        if fname is None: \n",
    "            print(\"embedding is random\")\n",
    "            return embeddings\n",
    "            \n",
    "        else:\n",
    "            word_vecs = {}\n",
    "            count = 0\n",
    "            with open(fname,encoding = 'utf-8') as f:\n",
    "                i = 0\n",
    "                for line in f:\n",
    "                    i += 1\n",
    "                    if i % 100000 == 0:\n",
    "                        print ('epch %d' % i)\n",
    "                    items = line.strip().split(' ')\n",
    "                    if len(items) == 2:\n",
    "                        vocab_size, embedding_size = items[0], items[1]\n",
    "                        print (vocab_size, embedding_size)\n",
    "                    else:\n",
    "                        word = items[0]\n",
    "                        if word in vocab:\n",
    "                            count += 1\n",
    "                            embeddings[vocab[word]] = items[1:]\n",
    "            print('there are {} words can be found in dict'.format(count))\n",
    "        return embeddings\n",
    "embedding_file = '../glove.6B.100d.txt'\n",
    "word_embedding = get_embedding(embedding_file,word_dict,dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 514  134  555 ...    0    0    0]\n",
      " [   2  865 3204 ...    0    0    0]\n",
      " [ 546  134   17 ...    0    0    0]\n",
      " ...\n",
      " [1679  530  134 ...    0    0    0]\n",
      " [1679    3 3752 ...    0    0    0]\n",
      " [ 514   20  459 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# get batch_data\n",
    "\n",
    "train_file = os.path.join(data,'train')\n",
    "test_file = os.path.join(data,'test')\n",
    "df_train = pd.read_csv(train_file,sep = '\\t',names = ['s1','s2','s2_'],quoting = 3)\n",
    "df_test = pd.read_csv(test_file,sep = '\\t',names = ['s1','s2'],quoting = 3)\n",
    "batch_size = 64\n",
    "\n",
    "def triple_pair(row):\n",
    "    return pd.Series({'s1_id':convert_to_word_ids(row['s1']),'s2_id':convert_to_word_ids(row['s2']),'s2_neg_id':convert_to_word_ids(row['s2_'])})\n",
    "def point_pair(row):\n",
    "    return pd.Series({'s1_id':convert_to_word_ids(row['s1']),'s2_id':convert_to_word_ids(row['s2'])})\n",
    "def chunker(seq,size):\n",
    "    return (seq[pos:pos + size] for pos in range(0,len(seq),size))\n",
    "\n",
    "def cut(sentence):\n",
    "    return sentence.strip().lower().split()\n",
    "def convert_to_word_ids(sentence,max_len = 40):\n",
    "    indices = []\n",
    "    tokens = cut(sentence)\n",
    "    for word in tokens:\n",
    "        if word in word_dict:\n",
    "            indices.append(word_dict[word])\n",
    "        else:\n",
    "            continue\n",
    "    result = indices + [word_dict['UNK']] * (max_len - len(indices))\n",
    "\n",
    "    return result[:max_len]\n",
    "    \n",
    "    \n",
    "def batch_iter_pandas_train(df,batch_size):\n",
    "    \n",
    "#     shuffle the dataset\n",
    "    df = df.sample(frac = 1).reset_index(drop = True)     \n",
    "    batches = chunker(df,batch_size)\n",
    "    \n",
    "    for b in batches:\n",
    "        q = b['s1_id']\n",
    "        a = b['s2_id']\n",
    "        a_ = b['s2_neg_id']\n",
    "        yield(np.array(q.tolist()),np.array(a.tolist()),np.array(a_.tolist()))\n",
    "def batch_iter_pandas_test(df,batch_size):\n",
    "    batches = chunker(df,batch_size)\n",
    "    \n",
    "    for b in batches:\n",
    "        q = b['s1_id']\n",
    "        a = b['s2_id']\n",
    "        yield(np.array(q.tolist()),np.array(a.tolist()))\n",
    "        \n",
    "# Convert the sentence to ids\n",
    "df_train_pairs = df_train.apply(triple_pair,axis = 1)\n",
    "\n",
    "batch_train = batch_iter_pandas_train(df_train_pairs,batch_size)\n",
    "for q,a,a_ in batch_train:\n",
    "    print(q)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-10-91d85c9d6c67>:46: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 利用place_holder\n",
    "\n",
    "q = tf.placeholder(tf.int32,[None,None],name = 'q')\n",
    "a = tf.placeholder(tf.int32,[None,None],name = 'a')\n",
    "a_ = tf.placeholder(tf.int32,[None,None],name = 'a_')\n",
    "\n",
    "\n",
    "margin = 0.05\n",
    "learning_rate = 0.001\n",
    "embedding_size = 100\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 64\n",
    "# get embedding\n",
    "\n",
    "embed = tf.Variable(word_embedding,trainable = False,name = 'fix_embedding')\n",
    "\n",
    "# get the embed of the sentence\n",
    "\n",
    "q_embed = tf.nn.embedding_lookup(embed,q)\n",
    "a_embed = tf.nn.embedding_lookup(embed,a)\n",
    "a_neg_embed = tf.nn.embedding_lookup(embed,a_)\n",
    "\n",
    "# # encode the sentence\n",
    "# q_encode = tf.reduce_sum(q_embed,1,name = 'q_encode')\n",
    "# a_encode = tf.reduce_sum(a_embed,1,name = 'a_encode')\n",
    "# a_neg_encode = tf.reduce_sum(a_neg_embed,1,name = 'a_neg_encode')\n",
    "\n",
    "# convolution the sentence\n",
    "q_embed_expand = tf.expand_dims(q_embed,-1,'q_expand')\n",
    "a_embed_expand = tf.expand_dims(a_embed,-1,'a_expand')\n",
    "a_neg_embed_encode = tf.expand_dims(a_neg_embed,-1,'a_neg_expand')\n",
    "\n",
    "\n",
    "\n",
    "output_q = []\n",
    "output_a = []\n",
    "output_a_neg = []\n",
    "for filter_size in filter_sizes:\n",
    "    with tf.name_scope('conv-pool-%s' % filter_size):\n",
    "    # [batch,sequence_len,embedding,1]\n",
    "        conv1 = tf.layers.conv2d(q_embed_expand,num_filters,strides = [1,1],\n",
    "                            kernel_size = [filter_size,embedding_size],\n",
    "                            padding = 'VALID',\n",
    "                            reuse = False,\n",
    "                            activation = tf.nn.relu,\n",
    "                            name = 'conv_{}'.format(str(filter_size)))\n",
    "\n",
    "\n",
    "        pool1 = tf.reduce_mean(conv1,1,name = 'pool1',keep_dims=True)\n",
    "        output_q.append(pool1)\n",
    "                                 \n",
    "        conv2 = tf.layers.conv2d(a_embed_expand,num_filters,strides = [1,1],\n",
    "                            kernel_size = [filter_size,embedding_size],\n",
    "                            padding = 'VALID',\n",
    "                            reuse = True,\n",
    "                            activation = tf.nn.relu,\n",
    "                            name = 'conv_{}'.format(str(filter_size)))\n",
    "\n",
    "        pool2 = tf.reduce_mean(conv2,1,name = 'pool2',keep_dims=True)\n",
    "        \n",
    "        output_a.append(pool2)\n",
    "\n",
    "\n",
    "        conv3 = tf.layers.conv2d(a_neg_embed_encode,num_filters,strides = [1,1],\n",
    "                                kernel_size = [filter_size,embedding_size],\n",
    "                                padding = 'VALID',\n",
    "                                reuse = True,\n",
    "                                activation = tf.nn.relu,\n",
    "                                name = 'conv_{}'.format(str(filter_size)))\n",
    "\n",
    "        pool3 = tf.reduce_mean(conv3,1,name = 'pool3',keep_dims=True)\n",
    "        output_a_neg.append(pool3)\n",
    "\n",
    "num_filters_total = len(filter_sizes) * num_filters\n",
    "\n",
    "h_pool_q = tf.concat(output_q,3)\n",
    "h_pool_a = tf.concat(output_a,3)\n",
    "h_pool_a_neg = tf.concat(output_a_neg,3)\n",
    "\n",
    "encode_q = tf.reshape(h_pool_q,[-1,num_filters_total])\n",
    "encode_a = tf.reshape(h_pool_a,[-1,num_filters_total])\n",
    "encode_a_neg = tf.reshape(h_pool_a_neg,[-1,num_filters_total])                                 \n",
    "                                 \n",
    "                            \n",
    "                            \n",
    "# create the loss\n",
    "\n",
    "def consin_score(q,a):\n",
    "    q_norm = tf.nn.l2_normalize(q,1)\n",
    "    a_norm = tf.nn.l2_normalize(a,1)\n",
    "    \n",
    "    return tf.cast(tf.reduce_sum(tf.multiply(q_norm,a_norm),1),tf.float32)\n",
    "score_q_a = consin_score(encode_q,encode_a)\n",
    "score_q_neg = consin_score(encode_q,encode_a_neg) \n",
    "\n",
    "loss = tf.reduce_mean(tf.math.maximum(0.0,tf.math.subtract(margin,tf.math.subtract(score_q_a,score_q_neg))))\n",
    "\n",
    "correct = tf.equal(0.0,loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name = 'acc')\n",
    "\n",
    "# create the train_op\n",
    "\n",
    "global_step = tf.Variable(0, name = \"global_step\", trainable = False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.1304152011871338 pos:0.8110812902450562 neg:0.8412067890167236\n",
      "loss:0.09701573848724365 pos:0.9175577163696289 neg:0.9515644907951355\n",
      "loss:0.05771539360284805 pos:0.9826871156692505 neg:0.9903937578201294\n",
      "loss:0.05463765561580658 pos:0.983115553855896 neg:0.9877532124519348\n",
      "loss:0.058633144944906235 pos:0.9772401452064514 neg:0.985873281955719\n",
      "loss:0.05442947521805763 pos:0.9853833317756653 neg:0.9892674088478088\n",
      "loss:0.05454438179731369 pos:0.9846780300140381 neg:0.9892224073410034\n",
      "loss:0.05242455378174782 pos:0.9912303686141968 neg:0.9936549663543701\n",
      "loss:0.052488237619400024 pos:0.9915428161621094 neg:0.9940310716629028\n",
      "loss:0.051095928996801376 pos:0.9932905435562134 neg:0.9943863749504089\n",
      "loss:0.05204382911324501 pos:0.9943829774856567 neg:0.9964268207550049\n",
      "loss:0.052139781415462494 pos:0.9953771829605103 neg:0.9975168704986572\n",
      "loss:0.050998322665691376 pos:0.9961888790130615 neg:0.9971871376037598\n",
      "loss:0.05134999006986618 pos:0.9963234663009644 neg:0.997673511505127\n",
      "loss:0.05000265687704086 pos:0.997940719127655 neg:0.9979434013366699\n",
      "loss:0.05062924325466156 pos:0.997490406036377 neg:0.9981197118759155\n",
      "loss:0.051316723227500916 pos:0.9971569180488586 neg:0.9984736442565918\n",
      "loss:0.05143513157963753 pos:0.9974093437194824 neg:0.9988445043563843\n",
      "loss:0.050357885658741 pos:0.9980033040046692 neg:0.9983612298965454\n",
      "loss:0.050279565155506134 pos:0.9988774061203003 neg:0.9991570115089417\n",
      "loss:0.050953298807144165 pos:0.9982622265815735 neg:0.9992154836654663\n",
      "loss:0.05032384768128395 pos:0.998717188835144 neg:0.9990410208702087\n",
      "loss:0.05052684247493744 pos:0.9986463785171509 neg:0.9991732239723206\n",
      "loss:0.05034243315458298 pos:0.9991121888160706 neg:0.9994546175003052\n",
      "loss:0.05041183903813362 pos:0.9988739490509033 neg:0.9992856979370117\n",
      "loss:0.05001399666070938 pos:0.9991231560707092 neg:0.9991371631622314\n",
      "loss:0.050538428127765656 pos:0.9989069104194641 neg:0.9994453191757202\n",
      "loss:0.05017390102148056 pos:0.9991686940193176 neg:0.9993425607681274\n",
      "loss:0.05020269751548767 pos:0.9989398717880249 neg:0.999142587184906\n",
      "loss:0.050460126250982285 pos:0.9989203214645386 neg:0.9993804693222046\n",
      "loss:0.05011816322803497 pos:0.9991154670715332 neg:0.999233603477478\n",
      "loss:0.05018739402294159 pos:0.9989469647407532 neg:0.9991343021392822\n",
      "loss:0.050436832010746 pos:0.9990036487579346 neg:0.9994404315948486\n",
      "loss:0.05005783587694168 pos:0.9990817308425903 neg:0.9991395473480225\n",
      "loss:0.05013465881347656 pos:0.9992599487304688 neg:0.9993946552276611\n",
      "loss:0.049888744950294495 pos:0.9993237257003784 neg:0.9992125630378723\n",
      "loss:0.05004691705107689 pos:0.9991192817687988 neg:0.9991661906242371\n",
      "loss:0.05016547441482544 pos:0.9991897344589233 neg:0.9993551969528198\n",
      "loss:0.05006464570760727 pos:0.999262809753418 neg:0.9993274211883545\n",
      "loss:0.05032528191804886 pos:0.9991605877876282 neg:0.9994858503341675\n",
      "loss:0.050296567380428314 pos:0.999184250831604 neg:0.9994807839393616\n",
      "loss:0.05041747912764549 pos:0.9992433190345764 neg:0.9996607899665833\n",
      "loss:0.050034552812576294 pos:0.999447226524353 neg:0.9994817972183228\n",
      "loss:0.05021429434418678 pos:0.9993135929107666 neg:0.9995279312133789\n",
      "loss:0.04991511255502701 pos:0.9996190071105957 neg:0.9995341300964355\n",
      "loss:0.05001615732908249 pos:0.9994954466819763 neg:0.9995115995407104\n",
      "loss:0.050220053642988205 pos:0.9994144439697266 neg:0.9996345043182373\n",
      "loss:0.05004158988595009 pos:0.9995233416557312 neg:0.9995648860931396\n",
      "loss:0.05015059560537338 pos:0.999385416507721 neg:0.9995359778404236\n",
      "loss:0.050090860575437546 pos:0.9994308352470398 neg:0.9995216727256775\n",
      "loss:0.050260305404663086 pos:0.9992474317550659 neg:0.9995077252388\n",
      "loss:0.05028870701789856 pos:0.9993607997894287 neg:0.9996495246887207\n",
      "loss:0.0501602478325367 pos:0.9994641542434692 neg:0.9996243715286255\n",
      "loss:0.050141364336013794 pos:0.9994539618492126 neg:0.9995953440666199\n",
      "loss:0.050085362046957016 pos:0.9994832277297974 neg:0.9995685815811157\n",
      "loss:0.050014786422252655 pos:0.9994187355041504 neg:0.9994335174560547\n",
      "loss:0.0501050129532814 pos:0.9994851350784302 neg:0.9995900988578796\n",
      "loss:0.05007197707891464 pos:0.9994262456893921 neg:0.9994982481002808\n",
      "loss:0.050067588686943054 pos:0.9993748664855957 neg:0.999442458152771\n",
      "loss:0.050088733434677124 pos:0.9994232654571533 neg:0.9995120167732239\n",
      "loss:0.050337646156549454 pos:0.9992756843566895 neg:0.999613344669342\n",
      "loss:0.050177354365587234 pos:0.9994617700576782 neg:0.9996390342712402\n",
      "loss:0.05010657384991646 pos:0.9994679689407349 neg:0.9995745420455933\n",
      "loss:0.050041697919368744 pos:0.9994906187057495 neg:0.9995323419570923\n",
      "loss:0.05005166679620743 pos:0.9994879961013794 neg:0.9995396137237549\n",
      "loss:0.04991769790649414 pos:0.9995570182800293 neg:0.9994746446609497\n",
      "loss:0.049985431134700775 pos:0.9994230270385742 neg:0.9994083642959595\n",
      "loss:0.04993776977062225 pos:0.9995834827423096 neg:0.9995211362838745\n",
      "loss:0.04995449259877205 pos:0.9994184970855713 neg:0.9993730187416077\n",
      "loss:0.04977443814277649 pos:0.9995787143707275 neg:0.9993531703948975\n",
      "loss:0.05035945773124695 pos:0.9991958141326904 neg:0.999555230140686\n",
      "loss:0.04995676130056381 pos:0.9992223978042603 neg:0.9991791248321533\n",
      "loss:0.05013107508420944 pos:0.999261736869812 neg:0.9993927478790283\n",
      "loss:0.05020320042967796 pos:0.9991292357444763 neg:0.9993324279785156\n",
      "loss:0.05022381991147995 pos:0.9993157386779785 neg:0.9995396137237549\n",
      "loss:0.04997289925813675 pos:0.9993494749069214 neg:0.9993224143981934\n",
      "loss:0.04992390424013138 pos:0.9995306730270386 neg:0.9994544982910156\n",
      "loss:0.049956005066633224 pos:0.9994794130325317 neg:0.9994354248046875\n",
      "loss:0.050109803676605225 pos:0.9994901418685913 neg:0.9995998740196228\n",
      "loss:0.05010134354233742 pos:0.9993664026260376 neg:0.9994677305221558\n",
      "loss:0.049959179013967514 pos:0.9994543790817261 neg:0.9994135499000549\n",
      "loss:0.04999382421374321 pos:0.9994522929191589 neg:0.9994460344314575\n",
      "loss:0.04996511712670326 pos:0.9994709491729736 neg:0.9994360208511353\n",
      "loss:0.05005301907658577 pos:0.9994086027145386 neg:0.9994615912437439\n",
      "loss:0.050174959003925323 pos:0.9992406964302063 neg:0.9994156360626221\n",
      "loss:0.05010875687003136 pos:0.9992785453796387 neg:0.9993873238563538\n",
      "loss:0.04990360140800476 pos:0.9992229342460632 neg:0.9991264939308167\n",
      "loss:0.05025961250066757 pos:0.9989309906959534 neg:0.999190628528595\n",
      "loss:0.05009584128856659 pos:0.9991142749786377 neg:0.9992101192474365\n",
      "loss:0.04985577613115311 pos:0.9992578029632568 neg:0.9991135597229004\n",
      "loss:0.05024803429841995 pos:0.998930811882019 neg:0.9991788268089294\n",
      "loss:0.05014076828956604 pos:0.9991586208343506 neg:0.9992992877960205\n",
      "loss:0.05038455128669739 pos:0.9987927079200745 neg:0.9991772770881653\n",
      "loss:0.04972948133945465 pos:0.9994521737098694 neg:0.9991816282272339\n",
      "loss:0.04983435571193695 pos:0.9995207786560059 neg:0.9993551969528198\n",
      "loss:0.05000735819339752 pos:0.9993399977684021 neg:0.9993473291397095\n",
      "loss:0.05003716051578522 pos:0.9992966055870056 neg:0.9993337392807007\n",
      "loss:0.049650825560092926 pos:0.9994494915008545 neg:0.9991002678871155\n",
      "loss:0.049982115626335144 pos:0.999355673789978 neg:0.9993377923965454\n",
      "loss:0.049952246248722076 pos:0.999290943145752 neg:0.9992431402206421\n",
      "loss:0.0500631183385849 pos:0.9992023706436157 neg:0.9992654323577881\n",
      "loss:0.05011192709207535 pos:0.9992157220840454 neg:0.9993276000022888\n",
      "loss:0.049783434718847275 pos:0.9993724822998047 neg:0.9991558790206909\n",
      "loss:0.050045646727085114 pos:0.9993507266044617 neg:0.9993963837623596\n",
      "loss:0.0496678501367569 pos:0.9994182586669922 neg:0.9990861415863037\n",
      "loss:0.04986726865172386 pos:0.99924236536026 neg:0.9991096258163452\n",
      "loss:0.04964892938733101 pos:0.9994101524353027 neg:0.9990590810775757\n",
      "loss:0.04998967796564102 pos:0.9990787506103516 neg:0.9990684390068054\n",
      "loss:0.05006341636180878 pos:0.9988188743591309 neg:0.9988823533058167\n",
      "loss:0.04968305677175522 pos:0.9986370801925659 neg:0.9983200430870056\n",
      "loss:0.04974178224802017 pos:0.9988890886306763 neg:0.9986308813095093\n",
      "loss:0.05034791678190231 pos:0.9980701208114624 neg:0.9984180331230164\n",
      "loss:0.05017388239502907 pos:0.9982079863548279 neg:0.9983818531036377\n",
      "loss:0.04991248995065689 pos:0.9982141256332397 neg:0.9981265068054199\n",
      "loss:0.04966460168361664 pos:0.9986196756362915 neg:0.9982843399047852\n",
      "loss:0.049899470061063766 pos:0.9989272356033325 neg:0.9988266825675964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.04980599135160446 pos:0.9986735582351685 neg:0.9984794855117798\n",
      "loss:0.05006594583392143 pos:0.9987421035766602 neg:0.9988080263137817\n",
      "loss:0.04929134249687195 pos:0.9990805983543396 neg:0.9983718395233154\n",
      "loss:0.049870509654283524 pos:0.9989293217658997 neg:0.9987998604774475\n",
      "loss:0.04913055896759033 pos:0.9991165399551392 neg:0.9982471466064453\n",
      "loss:0.049807026982307434 pos:0.998977541923523 neg:0.9987846612930298\n",
      "loss:0.05032399296760559 pos:0.9979656934738159 neg:0.9982895851135254\n",
      "loss:0.0504387728869915 pos:0.9977156519889832 neg:0.9981544017791748\n",
      "loss:0.05033506453037262 pos:0.9978708028793335 neg:0.9982057809829712\n",
      "loss:0.050032515078783035 pos:0.9984179735183716 neg:0.9984504580497742\n",
      "loss:0.04976886510848999 pos:0.9988811016082764 neg:0.9986499547958374\n",
      "loss:0.04980524629354477 pos:0.9989550709724426 neg:0.9987603425979614\n",
      "loss:0.04984576255083084 pos:0.9990796446800232 neg:0.9989253282546997\n",
      "loss:0.049390628933906555 pos:0.9992243051528931 neg:0.9986149072647095\n",
      "loss:0.04972898215055466 pos:0.9988441467285156 neg:0.9985730648040771\n",
      "loss:0.05036180466413498 pos:0.9982795715332031 neg:0.9986413717269897\n",
      "loss:0.050263114273548126 pos:0.9982774257659912 neg:0.9985405206680298\n",
      "loss:0.05036899819970131 pos:0.9985511898994446 neg:0.998920202255249\n",
      "loss:0.05050734430551529 pos:0.9980883598327637 neg:0.9985957145690918\n",
      "loss:0.04980209842324257 pos:0.9988673329353333 neg:0.9986693859100342\n",
      "loss:0.0499357245862484 pos:0.9989629983901978 neg:0.9988987445831299\n",
      "loss:0.04969145357608795 pos:0.9994486570358276 neg:0.9991400241851807\n",
      "loss:0.0505499504506588 pos:0.9987196922302246 neg:0.9992696046829224\n",
      "loss:0.04939952865242958 pos:0.999330997467041 neg:0.998730480670929\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for s1,s2,s2_ in batch_train:\n",
    "\n",
    "        feed_dict = {\n",
    "            q:s1,\n",
    "            a:s2,\n",
    "            a_:s2_\n",
    "        }\n",
    "        _,step,losses,pos_score,neg_score = sess.run([train_op,global_step,loss,score_q_a,score_q_neg],feed_dict = feed_dict)\n",
    "        print('loss:{} pos:{} neg:{}'.format(losses,np.mean(pos_score),np.mean(neg_score)))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pairs = df_test.apply(point_pair,axis = 1)\n",
    "\n",
    "batch_test = batch_iter_pandas_test(df_test_pairs,100)\n",
    "\n",
    "test_score = []\n",
    "for s1,s2 in batch_test:\n",
    "    feed_dict = {\n",
    "        q:s1,\n",
    "        a:s2\n",
    "    }\n",
    "    score = sess.run(score_q_a,feed_dict = feed_dict)\n",
    "    test_score.extend(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3655099485131332, 0.36623011047365434)\n",
      "(0.4033543716323824, 0.4097187726542667)\n"
     ]
    }
   ],
   "source": [
    "import evaluation\n",
    "\n",
    "test_raw_file = os.path.join(data,'test.txt')\n",
    "df_raw_test = pd.read_csv(test_raw_file,sep = '\\t',names = ['s1','s2','flag'],quoting = 3)\n",
    "assert len(df_test) == len(test_score)\n",
    "print(evaluation.evaluationBypandas(df_raw_test,test_score))\n",
    "print(evaluation.evaluationBypandas(df_raw_test,np.random.randn(len(df_raw_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
